{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from dnn_app_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "#from dnn_app_utils_v3 import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_dataset = h5py.File('train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])  # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:])  # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:])  # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:])  # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:])  # the list of classes\n",
    "\n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Z = np.dot(W, A) + b\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    # print('parameters_val ', parameters.keys())\n",
    "    # print('L is ', L)\n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    # print('AL is ', AL)\n",
    "    # print('Y is ', Y)\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    cost = -np.sum(Y*np.log(AL) + (1-Y)*np.log(1-AL))/m\n",
    "    # print('cost is ',cost)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    dW = (np.dot(dZ,cache[0].T))/m\n",
    "    db = np.sum(dZ, axis = 1, keepdims=True)/m\n",
    "    dA_prev = np.dot(cache[1].T,dZ)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layersp\n",
    "    #print('L is ',L)\n",
    "    m = AL.shape[1]\n",
    "    #print('length of cache is ',len(caches))\n",
    "    #print('cache is ',caches)\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = caches[-1]\n",
    "    #print('current cache shape is ', len(current_cache))\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_backward(sigmoid_backward(dAL,current_cache[1]),current_cache[0])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp =  linear_backward(relu_backward(grads[\"dA\" + str(l + 1)], current_cache[1]), current_cache[0])\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l+1)] = dW_temp\n",
    "        grads[\"db\" + str(l+1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (12288, 209)\n",
      "test_x's shape: (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [12288, 20, 7, 5, 1] #  4-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_layer_model\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. (≈ 1 line of code)\n",
    "    ### START CODE HERE ###\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "        #print('learning rate ', learning_rate, 'iteration ',i)\n",
    "        # print('para ',parameters['W1'][0,0], 'iteration ',i)\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        #print('cpst is ',cost)\n",
    "        ### END CODE HERE ###\n",
    "        #print('cache length is: ', len(caches))\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693148\n",
      "Cost after iteration 100: 0.678011\n",
      "Cost after iteration 200: 0.667600\n",
      "Cost after iteration 300: 0.660422\n",
      "Cost after iteration 400: 0.655458\n",
      "Cost after iteration 500: 0.652013\n",
      "Cost after iteration 600: 0.649616\n",
      "Cost after iteration 700: 0.647942\n",
      "Cost after iteration 800: 0.646770\n",
      "Cost after iteration 900: 0.645947\n",
      "Cost after iteration 1000: 0.645368\n",
      "Cost after iteration 1100: 0.644961\n",
      "Cost after iteration 1200: 0.644673\n",
      "Cost after iteration 1300: 0.644469\n",
      "Cost after iteration 1400: 0.644325\n",
      "Cost after iteration 1500: 0.644223\n",
      "Cost after iteration 1600: 0.644151\n",
      "Cost after iteration 1700: 0.644100\n",
      "Cost after iteration 1800: 0.644063\n",
      "Cost after iteration 1900: 0.644037\n",
      "Cost after iteration 2000: 0.644019\n",
      "Cost after iteration 2100: 0.644006\n",
      "Cost after iteration 2200: 0.643997\n",
      "Cost after iteration 2300: 0.643990\n",
      "Cost after iteration 2400: 0.643985\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
